보팅(Voting)
하드보팅(Hard voting) : 다수결
소프트 보팅(Soft voting) : 각 확률의 평균
공통점 : 여러 개의 모델이 투표 혹은 평균을 통해 최종 예측결과를 선정

보팅(Voting) : 서로 다른 모델을 결합
배깅(Bagging) : 같은 종류의 모델을 결합(데이터 샘플링을 다르게, 중첩허용)

결정 트리
- 직관적이어서 결과를 쉽게 이해할 수 있음
- 트리가 깊어질수록 과대적합이 되기 쉬움

랜덤 포레스트
주요 매개변수(HyperOarameter)
RandomForestClassifier(n_estimators,
max_features, random_state)

n_estimators : 트리의 개수
선택할 특징의 최대 수 : max_features
선택할 데이터의 시드 : random_state

결정 트리 매개변수(HyperOarameter)
- 트리의 최대 깊이 : max_depth
- 말단 노드 최대 개수 : max_leaf_nodes
- 말단 노드가 되기 위한 최소 샘플 수 : min_samples_leaf

랜덤 포레스트(Random forest) 특징
- 결정 트리 모델의 과대적합을 통계적 방법으로 해소
- 결정 트리 모델처럼 쉽고 직관적임
- 부스팅 방식에 비해 빠른 수행 속도
- 모델 튜닝을 위한 시간이 많이 필요(하이퍼 파라미터의 종류가 많음)
- 큰 데이터 세트에도 잘 동작하지만 트리 개수가 많라질수록 시간이 오래 걸림

XG Boosting 특징
- GBM 의 단점 : 느림, 과대적합 문제
- GBM 보다 빠름 -> Early Stopping 제공
- 과대적합 방지를 위한 규제 포함
- 병렬로 빠른 학습이 가능

SVM(Support Vector Machine)
목적 : 두 레이블 간 거리를 최대화 하는것(여백 = Margin)
- 신경망보다 사용이 간결하며 분류나 회귀 분석에 사용 가능하지만, 분류에서
  주로 사용되고 있는 기법
- 기존의 분류 모델에서 더 나아가서 다른 레이블 사이에
  존재하는 여백을 최대화 하려는 목적으로 설계되었다

- Hiperplane(초평면) : 어떤 N차원 공간에서 한 차원 낮은 N-1차원의 subspace
- support vector : data set 안에 포함되어 있는 두 가지 레이블로
  구분되는 data 중 최외각에 있는 샘플들의 vector
- margin : support vector를 통해 구한 두 레이블 사이의 거리
  (얘를 최대화하는 것이 목적)

Hyperplane을 잘 분류하는 기준
- 어느 한 쪽에 치우치지 않도록 분류
- 양쪽 데이터와 균등한 위치에 분휴 기준 세우기

SVM 분류 방법
1.두 레이블 중 어느 하나에 속한 데이터 집합이 주어짐
2 주어진 dataset을 바탕으로 새로운 데이터가 어느 레이블에 속하는지
  판단하는 비확률적 선형분류모델을 만듦
3.만들어진 분휴모델은 데이터가 사상된 공간에서 경계로 표현되는데
  SVM알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾음

SVM - 이상치 허용
하이퍼파라미터(Hyper parameter) : C
- Hard margin : 이상치 거의 허용하지 않음
- Soft margin : 이상치 어느정도는 허용

C값이 클때 : 이상치 허용 X
C값이 작을때 : 이상치 허용 O
 -> 모델이 과대적합되지 않고 어느정도 일반화된
    모델을 만들겠다.
 
gamma - 가우시안 커널
gamma 조절 -> 하나의 데이터 각각이 
              얼만큼의 영향을 가질지 결정
gamma 값이 클수록 영향력도 커짐
gamma 값이 작을수록 영향력도 작아짐

C값과 gamma값이 둘다 클때 과대적합의 위험
C값과 gamma값이 둘다 작을때 과소적합의 위험

이상치 허용
허용 O : margin 커짐
허용 X : margin 작아짐

SVM(Support vector Machine)
- 회귀, 본류, 이상치 탐지 등에 사용되는 지도학습 방법
- 클래스 사이의 경계에 위치한 데이터 포인트를 support vector라고함
- 각 support vector가 클래스 사이의 결정 경계를 구분하는데 얼마나 중요한지를 학습
- 각 support bector 사이의 마진이 가장 큰 방향으로 학습
- support vector 까지의 거리와 support bector의 중요도를 기반으로 예측을 수행

정리
- SVM은 서포트 벡터(support vectors)를 사용해서 결정 경계(Decision Boundary)를 정의하고,
  분류되지 않은 점을 해당 결정 경계와 비교해서 분류한다.
- SVM은 허용 가능한 오차 범위 내에서 가능한 최대 마진을 만들려고 한다.
- SVM에서는 선형으로 분리할 수 없는 점들을 분류하기 위해 커널(kernel)을 사용한다.
- 커널(kernel)은 원래 가지고 있는 데이터를더 높은 차원의 데이터로 변환한다. 
- RBF커널에는 파라미터 감마(gamma)가 있다. 감마가 너무 크면 학습 데이터에 너무 의존해서
  오버피팅이 발생할 수 있다.














